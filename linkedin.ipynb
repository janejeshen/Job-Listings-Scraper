{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading requests library\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading configuration\n",
    "with open(\"config.json\",'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_portals = config[\"job_portal\"]\n",
    "job_title = config[\"job_title\"]\n",
    "locations = config[\"location\"]\n",
    "db_config = config[\"database\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to MySQL\n",
    "conn = mysql.connector.connect(\n",
    "    host=db_config[\"host\"],\n",
    "    user=db_config[\"user\"],\n",
    "    password=db_config[\"password\"],\n",
    "    database=db_config[\"database\"]\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database available\n"
     ]
    }
   ],
   "source": [
    "# Checking if the database job_listings is available\n",
    "cursor.execute(\"SHOW DATABASES\")\n",
    "\n",
    "# Fetching all databases\n",
    "databases = cursor.fetchall()\n",
    "\n",
    "database_names = [db[0] for db in databases]\n",
    "if \"job_listings\" in database_names:\n",
    "    print(\"Database available\")\n",
    "else:\n",
    "    print(\"Database not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a table for jobs in job listings database\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS jobs (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            job_title VARCHAR(255),\n",
    "            job_company VARCHAR(255),\n",
    "            job_description VARCHAR(255),\n",
    "            job_location VARCHAR(255),\n",
    "            link VARCHAR(255),\n",
    "            post_date VARCHAR(255)\n",
    "            )\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching data fron linkedin\n",
    "def fetch_jobs_from_linkedib(job_title,location):\n",
    "\n",
    "    # linkedin url\n",
    "    base_url = \"https://www.linkedin.com/jobs/search\"\n",
    "    url = f\"{base_url}?keywords={job_title.replace(' ', '%20')}&location={locations.replace(' ', '%20')}\"\n",
    "\n",
    "\n",
    "    # Setting up selenium webdriver\n",
    "    driver=webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    # waiting for the job listings to load\n",
    "    WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, 'base-card')))\n",
    "\n",
    "    # Parsing the html content using beautifulsoup\n",
    "    soup = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "\n",
    "    # Finding all the job containers\n",
    "    job_cards = soup.find_all(\"div\",class_=\"base-card\")\n",
    "\n",
    "    # Empty list that will hold the results\n",
    "    jobs = []\n",
    "    \n",
    "    # Looping through all job containers and extracting data\n",
    "    for job_card in job_cards:\n",
    "        # Extracting the job title\n",
    "        title = job_card.find(\"h3\",class_=\"base-search-card__title\").text.strip()\n",
    "        \n",
    "        # Extracting the companies name\n",
    "        company_name = job_card.find(\"h4\", class_=\"base-search-card__subtitle\").text.strip()\n",
    "        \n",
    "        # Extracting the location of the job\n",
    "        location=job_card.find(\"span\",class_=\"job-search-card__location\").text.strip()\n",
    "        \n",
    "        # Extracting the link to the job\n",
    "        link = job_card.find(\"a\")[\"href\"]\n",
    "        \n",
    "        # Extracting the date the job was posted\n",
    "        post_date = job_card.find(\"time\")[\"datetime\"]\n",
    "\n",
    "\n",
    "        # # Extracting the job id from the link\n",
    "        # job_id = link.split('/')[-1].split('?')[0]\n",
    "        # job_description_url = f\"{base_url}?keywords={job_title.replace(' ', '%20')}&location={location.replace(' ', '%20')}&position=1&currentJobId={job_id}\"\n",
    "\n",
    "\n",
    "        # \"\"\"\n",
    "        # Navigating to another page to extract the job description \n",
    "        # since they are on different pages with the other attributes.\n",
    "        # \"\"\"\n",
    "\n",
    "        # # Navigating to the page with the job description\n",
    "        # driver.get(job_description_url)\n",
    "\n",
    "        # # Waiting for the description to load\n",
    "        # WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME,\"\")))\n",
    "\n",
    "        # # Extracting job description\n",
    "        # description_soup = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "        # description = description_soup.find(\"div\",class_=\"show-more-less-html__markup\")\n",
    "\n",
    "        # Appending the results to the empty list\n",
    "        # jobs.append((job_id,title,company_name,description,location,link,post_date))\n",
    "        jobs.append((title,company_name,location,link,post_date))\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        After extracting the description ,the scraper must return to the main \n",
    "        page to extract the next job\n",
    "        \"\"\"\n",
    "        # # Returning to the main page\n",
    "        # driver.back()\n",
    "        \n",
    "        # # Waiting for the main page to load again\n",
    "        # WebDriverWait(driver,10).until(EC.presence_of_element_locate((By.CLASS_NAME,'base-card')))\n",
    "\n",
    "    driver.quit()\n",
    "    return jobs\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jobs_to_db(jobs):\n",
    "    for job in jobs:\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO jobs (id,job_title, job_company,job_location, link, post_date)\n",
    "            VALUES (%s, %s, %s, %s, %s,%s)\n",
    "        \"\"\", job)\n",
    "    conn.commit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
